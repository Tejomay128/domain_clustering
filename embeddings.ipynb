{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using torchaudio==0.9.0, but torchaudio>=0.10.0 is required to use MCTCTFeatureExtractor. This requires torch>=1.10.0. Please upgrade torch and torchaudio.\n",
      "You are using torch==1.9.0+cu111, but torch>=1.10.0 is required to use ViltModel. Please upgrade torch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model_info, data):\n",
    "    model_class, model_tokenizer, model_name = model_info\n",
    "    embeddings = {}\n",
    "    embeddings['sents'] = []\n",
    "    embeddings['vectors'] = []\n",
    "    tokenizer = model_tokenizer.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    count = 1\n",
    "    for sent in data:\n",
    "        embeddings['sents'].append(sent)\n",
    "        input_encoding = torch.tensor([tokenizer.encode(sent, add_special_tokens=True, max_length=128, truncation=True)])\n",
    "        input_encoding = input_encoding.to(device)\n",
    "        # print(input_encoding.shape)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_encoding)\n",
    "            final_hidden_layer = output[0].squeeze(dim=0)    # get the last hidden layer\n",
    "            final_hidden_layer = final_hidden_layer[:input_encoding.shape[1], :]\n",
    "            final_vector = np.array(final_hidden_layer.mean(dim=0).cpu())\n",
    "            # print(final_vector.shape)\n",
    "            embeddings['vectors'].append(final_vector)\n",
    "        if count%10000 == 0:\n",
    "            print(f\"{count} sentences done.\")\n",
    "        count += 1\n",
    "    \n",
    "    embeddings['sents'] = np.array(embeddings['sents'])\n",
    "    embeddings['vectors'] = np.array(embeddings['vectors'])\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def data_to_embeddings(dataset, model_info, split='train', lang='en'):\n",
    "    sentences = []\n",
    "    data = dataset[split]['translation']\n",
    "    random.shuffle(data)\n",
    "    data = data[:500000]\n",
    "    for parallel_sent in data:\n",
    "        sent = parallel_sent[lang]\n",
    "        sentences.append(sent)\n",
    "\n",
    "    embeddings = get_embeddings(model_info, sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-e9387d78518bc7f8\n",
      "Found cached dataset parquet (/home1/tejomay/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-e9387d78518bc7f8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dd3da887c24093a01606ee914bcc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences done.\n",
      "20000 sentences done.\n",
      "30000 sentences done.\n",
      "40000 sentences done.\n",
      "50000 sentences done.\n",
      "60000 sentences done.\n",
      "70000 sentences done.\n",
      "80000 sentences done.\n",
      "90000 sentences done.\n",
      "100000 sentences done.\n",
      "110000 sentences done.\n",
      "120000 sentences done.\n",
      "130000 sentences done.\n",
      "140000 sentences done.\n",
      "150000 sentences done.\n",
      "160000 sentences done.\n",
      "170000 sentences done.\n",
      "180000 sentences done.\n",
      "190000 sentences done.\n",
      "200000 sentences done.\n",
      "210000 sentences done.\n",
      "220000 sentences done.\n",
      "230000 sentences done.\n",
      "240000 sentences done.\n",
      "250000 sentences done.\n",
      "260000 sentences done.\n",
      "270000 sentences done.\n",
      "280000 sentences done.\n",
      "290000 sentences done.\n",
      "300000 sentences done.\n",
      "310000 sentences done.\n",
      "320000 sentences done.\n",
      "330000 sentences done.\n",
      "340000 sentences done.\n",
      "350000 sentences done.\n",
      "360000 sentences done.\n",
      "370000 sentences done.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "model_info = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "# print(len(dataset['train']['translation']))\n",
    "embeddings = data_to_embeddings(dataset, model_info, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sentences_en_distilbert.npy', embeddings['sents'])\n",
    "np.save('vectors_en_distilbert.npy', embeddings['vectors'])\n",
    "print(len(embeddings['sents']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
