{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = ['Automotive', 'Books', 'Music', 'Software', 'Baby']\n",
    "\n",
    "\n",
    "def readfile(path):\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines if line.strip() != '']\n",
    "    f.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def process_data():\n",
    "    dataset = {}\n",
    "    for domain in DOMAINS:\n",
    "        print(f\"Processing '{domain}' tagged data ....\")\n",
    "        dataset[domain] = {}\n",
    "        SENT_PATH = \"domain_dataset/\" + domain + \".txt\"\n",
    "        SENTIMENT_PATH = \"domain_dataset/\" + domain + \"_labels.txt\"\n",
    "        dataset[domain]['sents'] = readfile(SENT_PATH)\n",
    "        dataset[domain]['sentiment'] = readfile(SENTIMENT_PATH)\n",
    "    return dataset\n",
    "        \n",
    "\n",
    "def get_embeddings(model_info, data):\n",
    "    model_class, model_tokenizer, model_name = model_info\n",
    "    embeddings = {}\n",
    "    embeddings['sents'] = []\n",
    "    embeddings['vectors'] = []\n",
    "    embeddings['domain'] = []\n",
    "    embeddings['sentiment'] = []\n",
    "    tokenizer = model_tokenizer.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    count = 1\n",
    "    for sent in data:\n",
    "        embeddings['sents'].append(sent[0])\n",
    "        embeddings['domain'].append(sent[1])\n",
    "        embeddings['sentiment'].append(sent[2])\n",
    "        input_encoding = torch.tensor([tokenizer.encode(sent[0], add_special_tokens=True, max_length=128, truncation=True)])\n",
    "        input_encoding = input_encoding.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_encoding)\n",
    "            final_hidden_layer = output[0].squeeze(dim=0)    # get the last hidden layer\n",
    "            final_hidden_layer = final_hidden_layer[:input_encoding.shape[1], :]\n",
    "            final_vector = np.array(final_hidden_layer.mean(dim=0).cpu())\n",
    "            embeddings['vectors'].append(final_vector)\n",
    "        if count%2500 == 0:\n",
    "            print(f\"{count} sentences done.\")\n",
    "        count += 1\n",
    "    embeddings['sents'] = np.array(embeddings['sents'])\n",
    "    embeddings['vectors'] = np.array(embeddings['vectors'])\n",
    "    embeddings['domain'] = np.array(embeddings['domain'])\n",
    "    embeddings['sentiment'] = np.array(embeddings['sentiment'])\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def data_to_embeddings(dataset, model_info):\n",
    "    sentences = []\n",
    "    for domain in DOMAINS:\n",
    "        data = dataset[domain]\n",
    "        for sent, sentiment in zip(data['sents'], data['sentiment']):\n",
    "            labelled_sent = (sent, domain, sentiment)\n",
    "            sentences.append(labelled_sent)\n",
    "    random.shuffle(sentences)\n",
    "    embeddings = get_embeddings(model_info, sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'Automotive' tagged data ....\n",
      "Processing 'Books' tagged data ....\n",
      "Processing 'Music' tagged data ....\n",
      "Processing 'Software' tagged data ....\n",
      "Processing 'Baby' tagged data ....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5427739d9c492b8034f7ddf5f83aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85b1c43626c496d8741e9730b8e6b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home1/tejomay/.cache/huggingface/hub/models--openai-gpt/snapshots/b3ab1942f7090e287d001cec22331dfc2764acf0/vocab.json\n",
      "loading file merges.txt from cache at /home1/tejomay/.cache/huggingface/hub/models--openai-gpt/snapshots/b3ab1942f7090e287d001cec22331dfc2764acf0/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0576af13fbe74aaea0a64d42139b83d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--openai-gpt/snapshots/b3ab1942f7090e287d001cec22331dfc2764acf0/config.json\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"_name_or_path\": \"openai-gpt\",\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--openai-gpt/snapshots/b3ab1942f7090e287d001cec22331dfc2764acf0/config.json\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a39d8659e94814a3fd870935ca9c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /home1/tejomay/.cache/huggingface/hub/models--openai-gpt/snapshots/b3ab1942f7090e287d001cec22331dfc2764acf0/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing OpenAIGPTModel.\n",
      "\n",
      "All the weights of OpenAIGPTModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use OpenAIGPTModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 sentences done.\n",
      "5000 sentences done.\n",
      "7500 sentences done.\n",
      "10000 sentences done.\n",
      "12500 sentences done.\n",
      "15000 sentences done.\n",
      "17500 sentences done.\n",
      "20000 sentences done.\n",
      "22500 sentences done.\n",
      "25000 sentences done.\n",
      "27500 sentences done.\n",
      "30000 sentences done.\n",
      "32500 sentences done.\n",
      "35000 sentences done.\n",
      "37500 sentences done.\n",
      "40000 sentences done.\n",
      "42500 sentences done.\n",
      "45000 sentences done.\n",
      "47500 sentences done.\n",
      "50000 sentences done.\n"
     ]
    }
   ],
   "source": [
    "#RUN THIS FOR ORIGINAL DATASET\n",
    "\n",
    "dataset = process_data()\n",
    "# model_info = (BertModel, BertTokenizer, 'bert-base-uncased')\n",
    "# model_info = (RobertaModel, RobertaTokenizer, 'roberta-large')\n",
    "model_info = (OpenAIGPTModel,  OpenAIGPTTokenizer, 'openai-gpt')\n",
    "embeddings = data_to_embeddings(dataset, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('openaiGPT/sentences.npy', embeddings['sents'])\n",
    "np.save('openaiGPT/vectors.npy', embeddings['vectors'])\n",
    "np.save('openaiGPT/domain.npy', embeddings['domain'])\n",
    "np.save('openaiGPT/sentiment.npy', embeddings['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive 10000\n",
      "Books 10000\n",
      "Music 10000\n",
      "Software 10000\n",
      "Baby 10000\n"
     ]
    }
   ],
   "source": [
    "for domain in DOMAINS:\n",
    "    print(domain, len(dataset[domain]['sents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
