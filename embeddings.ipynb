{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = ['Automotive', 'Books', 'Music', 'Software', 'Baby']\n",
    "\n",
    "\n",
    "def readfile(path):\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines if line.strip() != '']\n",
    "    f.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def process_data():\n",
    "    dataset = {}\n",
    "    for domain in DOMAINS:\n",
    "        print(f\"Processing '{domain}' tagged data ....\")\n",
    "        dataset[domain] = {}\n",
    "        SENT_PATH = \"domain_dataset/\" + domain + \".txt\"\n",
    "        SENTIMENT_PATH = \"domain_dataset/\" + domain + \"_labels.txt\"\n",
    "        dataset[domain]['sents'] = readfile(SENT_PATH)\n",
    "        dataset[domain]['sentiment'] = readfile(SENTIMENT_PATH)\n",
    "    return dataset\n",
    "        \n",
    "\n",
    "def get_embeddings(model_info, data):\n",
    "    model_class, model_tokenizer, model_name = model_info\n",
    "    embeddings = {}\n",
    "    embeddings['sents'] = []\n",
    "    embeddings['vectors'] = []\n",
    "    embeddings['domain'] = []\n",
    "    embeddings['sentiment'] = []\n",
    "    tokenizer = model_tokenizer.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    count = 1\n",
    "    for sent in data:\n",
    "        embeddings['sents'].append(sent[0])\n",
    "        embeddings['domain'].append(sent[1])\n",
    "        embeddings['sentiment'].append(sent[2])\n",
    "        input_encoding = torch.tensor([tokenizer.encode(sent[0], add_special_tokens=True, max_length=128, truncation=True)])\n",
    "        input_encoding = input_encoding.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_encoding)\n",
    "            final_hidden_layer = output[0].squeeze(dim=0)    # get the last hidden layer\n",
    "            final_hidden_layer = final_hidden_layer[:input_encoding.shape[1], :]\n",
    "            final_vector = np.array(final_hidden_layer.mean(dim=0).cpu())\n",
    "            embeddings['vectors'].append(final_vector)\n",
    "        if count%2500 == 0:\n",
    "            print(f\"{count} sentences done.\")\n",
    "        count += 1\n",
    "    embeddings['sents'] = np.array(embeddings['sents'])\n",
    "    embeddings['vectors'] = np.array(embeddings['vectors'])\n",
    "    embeddings['domain'] = np.array(embeddings['domain'])\n",
    "    embeddings['sentiment'] = np.array(embeddings['sentiment'])\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def data_to_embeddings(dataset, model_info):\n",
    "    sentences = []\n",
    "    for domain in DOMAINS:\n",
    "        data = dataset[domain]\n",
    "        for sent, sentiment in zip(data['sents'], data['sentiment']):\n",
    "            labelled_sent = (sent, domain, sentiment)\n",
    "            sentences.append(labelled_sent)\n",
    "    random.shuffle(sentences)\n",
    "    embeddings = get_embeddings(model_info, sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'Automotive' tagged data ....\n",
      "Processing 'Books' tagged data ....\n",
      "Processing 'Music' tagged data ....\n",
      "Processing 'Software' tagged data ....\n",
      "Processing 'Baby' tagged data ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 sentences done.\n",
      "5000 sentences done.\n",
      "7500 sentences done.\n",
      "10000 sentences done.\n",
      "12500 sentences done.\n",
      "15000 sentences done.\n",
      "17500 sentences done.\n",
      "20000 sentences done.\n",
      "22500 sentences done.\n",
      "25000 sentences done.\n",
      "27500 sentences done.\n",
      "30000 sentences done.\n",
      "32500 sentences done.\n",
      "35000 sentences done.\n",
      "37500 sentences done.\n",
      "40000 sentences done.\n",
      "42500 sentences done.\n",
      "45000 sentences done.\n",
      "47500 sentences done.\n",
      "50000 sentences done.\n"
     ]
    }
   ],
   "source": [
    "#RUN THIS FOR ORIGINAL DATASET\n",
    "\n",
    "dataset = process_data()\n",
    "model_info = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "embeddings = data_to_embeddings(dataset, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sentences.npy', embeddings['sents'])\n",
    "np.save('vectors.npy', embeddings['vectors'])\n",
    "np.save('domain.npy', embeddings['domain'])\n",
    "np.save('sentiment.npy', embeddings['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive 10000\n",
      "Books 10000\n",
      "Music 10000\n",
      "Software 10000\n",
      "Sports 10000\n"
     ]
    }
   ],
   "source": [
    "for domain in DOMAINS:\n",
    "    print(domain, len(dataset[domain]['sents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
