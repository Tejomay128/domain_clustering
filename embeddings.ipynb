{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model_info, data):\n",
    "    model_class, model_tokenizer, model_name = model_info\n",
    "    embeddings = {}\n",
    "    embeddings['sents'] = []\n",
    "    embeddings['vectors'] = []\n",
    "    tokenizer = model_tokenizer.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    count = 1\n",
    "    for sent in data:\n",
    "        embeddings['sents'].append(sent)\n",
    "        input_encoding = torch.tensor([tokenizer.encode(sent, add_special_tokens=True, max_length=128, truncation=True)])\n",
    "        input_encoding = input_encoding.to(device)\n",
    "        # print(input_encoding.shape)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_encoding)\n",
    "            final_hidden_layer = output[0].squeeze(dim=0)    # get the last hidden layer\n",
    "            final_hidden_layer = final_hidden_layer[:input_encoding.shape[1], :]\n",
    "            final_vector = np.array(final_hidden_layer.mean(dim=0).cpu())\n",
    "            # print(final_vector.shape)\n",
    "            embeddings['vectors'].append(final_vector)\n",
    "        if count%10000 == 0:\n",
    "            print(f\"{count} sentences done.\")\n",
    "        count += 1\n",
    "    \n",
    "    embeddings['sents'] = np.array(embeddings['sents'])\n",
    "    embeddings['vectors'] = np.array(embeddings['vectors'])\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def data_to_embeddings(dataset, model_info, split='train', lang='hi'):\n",
    "    sentences = []\n",
    "    data = dataset[split]['translation']\n",
    "    random.shuffle(data)\n",
    "    data = data[:400000]\n",
    "    for parallel_sent in data:\n",
    "        sent = parallel_sent[lang]\n",
    "        sentences.append(sent)\n",
    "\n",
    "    embeddings = get_embeddings(model_info, sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I saw a boy with a telescope\", \"I like this music\"]\n",
    "model_info = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "embeddings = get_embeddings(model_info, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-911387c6837f8b91\n",
      "Found cached dataset parquet (C:/Users/tejom/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcf2dda20ca4321b7e8d90c9ad5c07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\tejom/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\043235d6088ecd3dd5fb5ca3592b6913fd516027\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\tejom/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\043235d6088ecd3dd5fb5ca3592b6913fd516027\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\tejom/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\043235d6088ecd3dd5fb5ca3592b6913fd516027\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\tejom/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\043235d6088ecd3dd5fb5ca3592b6913fd516027\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\tejom/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\043235d6088ecd3dd5fb5ca3592b6913fd516027\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences done.\n",
      "20000 sentences done.\n",
      "30000 sentences done.\n",
      "40000 sentences done.\n",
      "50000 sentences done.\n",
      "60000 sentences done.\n",
      "70000 sentences done.\n",
      "80000 sentences done.\n",
      "90000 sentences done.\n",
      "100000 sentences done.\n",
      "110000 sentences done.\n",
      "120000 sentences done.\n",
      "130000 sentences done.\n",
      "140000 sentences done.\n",
      "150000 sentences done.\n",
      "160000 sentences done.\n",
      "170000 sentences done.\n",
      "180000 sentences done.\n",
      "190000 sentences done.\n",
      "200000 sentences done.\n",
      "210000 sentences done.\n",
      "220000 sentences done.\n",
      "230000 sentences done.\n",
      "240000 sentences done.\n",
      "250000 sentences done.\n",
      "260000 sentences done.\n",
      "270000 sentences done.\n",
      "280000 sentences done.\n",
      "290000 sentences done.\n",
      "300000 sentences done.\n",
      "310000 sentences done.\n",
      "320000 sentences done.\n",
      "330000 sentences done.\n",
      "340000 sentences done.\n",
      "350000 sentences done.\n",
      "360000 sentences done.\n",
      "370000 sentences done.\n",
      "380000 sentences done.\n",
      "390000 sentences done.\n",
      "400000 sentences done.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.8 GiB for an array with shape (400000,) and data type <U7216",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model_info \u001b[39m=\u001b[39m (DistilBertModel, DistilBertTokenizer, \u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# print(len(dataset['train']['translation']))\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m embeddings \u001b[39m=\u001b[39m data_to_embeddings(dataset, model_info, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [5], line 40\u001b[0m, in \u001b[0;36mdata_to_embeddings\u001b[1;34m(dataset, model_info, split, lang)\u001b[0m\n\u001b[0;32m     37\u001b[0m     sent \u001b[39m=\u001b[39m parallel_sent[lang]\n\u001b[0;32m     38\u001b[0m     sentences\u001b[39m.\u001b[39mappend(sent)\n\u001b[1;32m---> 40\u001b[0m embeddings \u001b[39m=\u001b[39m get_embeddings(model_info, sentences)\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "Cell \u001b[1;32mIn [5], line 26\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(model_info, data)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcount\u001b[39m}\u001b[39;00m\u001b[39m sentences done.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 26\u001b[0m embeddings[\u001b[39m'\u001b[39m\u001b[39msents\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(embeddings[\u001b[39m'\u001b[39;49m\u001b[39msents\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     27\u001b[0m embeddings[\u001b[39m'\u001b[39m\u001b[39mvectors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(embeddings[\u001b[39m'\u001b[39m\u001b[39mvectors\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.8 GiB for an array with shape (400000,) and data type <U7216"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "model_info = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "# print(len(dataset['train']['translation']))\n",
    "embeddings = data_to_embeddings(dataset, model_info, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sentences_hi.npy', embeddings['sents'])\n",
    "np.save('vectors_hi.npy', embeddings['vectors'])\n",
    "print(len(embeddings['sents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM(data, num_clusters):\n",
    "    model = GaussianMixture(\n",
    "        n_components=num_clusters, \n",
    "        covariance_type='full', \n",
    "        max_iter=100, \n",
    "        random_state=0)\n",
    "\n",
    "    model.fit(data)\n",
    "    predictions = model.predict(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aab9dab3ac8d3b09c685e0e9affdb28b29d6e0a00c5ce1bd7d9a5e575eda31bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
