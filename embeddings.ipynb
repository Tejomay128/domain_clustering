{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using torchaudio==0.9.0, but torchaudio>=0.10.0 is required to use MCTCTFeatureExtractor. This requires torch>=1.10.0. Please upgrade torch and torchaudio.\n",
      "You are using torch==1.9.0+cu111, but torch>=1.10.0 is required to use ViltModel. Please upgrade torch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['it', 'koran', 'law', 'medical', 'subtitles']\n",
    "\n",
    "\n",
    "def get_sentences(path, sent_set='train', lang='en'):\n",
    "    print(f\"Preparing {sent_set} data ....\")\n",
    "    filepath = path + sent_set + '.' + lang\n",
    "    f = open(filepath, \"r\")\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    f.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def process_data(lang='en'):\n",
    "    dataset = {}\n",
    "    for label in LABELS:\n",
    "        print(f\"Processing '{label}' tagged data ....\")\n",
    "        dataset[label] = {}\n",
    "        PATH = \"multi_domain_new_split/\" + label + \"/\"\n",
    "        dataset[label]['train'] = get_sentences(PATH, 'train', lang)\n",
    "        dataset[label]['test'] = get_sentences(PATH, 'test', lang)\n",
    "        dataset[label]['dev'] = get_sentences(PATH, 'dev', lang)\n",
    "    return dataset\n",
    "        \n",
    "\n",
    "def get_embeddings(model_info, data):\n",
    "    model_class, model_tokenizer, model_name = model_info\n",
    "    embeddings = {}\n",
    "    embeddings['sents'] = []\n",
    "    embeddings['vectors'] = []\n",
    "    embeddings['labels'] = []\n",
    "    tokenizer = model_tokenizer.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    count = 1\n",
    "    for sent in data:\n",
    "        embeddings['sents'].append(sent[0])\n",
    "        embeddings['labels'].append(sent[1])\n",
    "        input_encoding = torch.tensor([tokenizer.encode(sent[0], add_special_tokens=True, max_length=128, truncation=True)])\n",
    "        input_encoding = input_encoding.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_encoding)\n",
    "            final_hidden_layer = output[0].squeeze(dim=0)    # get the last hidden layer\n",
    "            final_hidden_layer = final_hidden_layer[:input_encoding.shape[1], :]\n",
    "            final_vector = np.array(final_hidden_layer.mean(dim=0).cpu())\n",
    "            embeddings['vectors'].append(final_vector)\n",
    "        if count%10000 == 0:\n",
    "            print(f\"{count} sentences done.\")\n",
    "        count += 1\n",
    "    \n",
    "    embeddings['sents'] = np.array(embeddings['sents'])\n",
    "    embeddings['vectors'] = np.array(embeddings['vectors'])\n",
    "    embeddings['labels'] = np.array(embeddings['labels'])\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def data_to_embeddings(dataset, model_info, split='train'):\n",
    "    sentences = []\n",
    "    for label in LABELS:\n",
    "        data = dataset[label][split]\n",
    "        if len(data) > 150000:\n",
    "            data = dataset[label][split][:150000]\n",
    "        \n",
    "        for sent in data:\n",
    "            labelled_sent = (sent, label)\n",
    "            sentences.append(labelled_sent)\n",
    "    random.shuffle(sentences)\n",
    "    embeddings = get_embeddings(model_info, sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'it' tagged data ....\n",
      "Preparing train data ....\n",
      "Preparing test data ....\n",
      "Preparing dev data ....\n",
      "Processing 'koran' tagged data ....\n",
      "Preparing train data ....\n",
      "Preparing test data ....\n",
      "Preparing dev data ....\n",
      "Processing 'law' tagged data ....\n",
      "Preparing train data ....\n",
      "Preparing test data ....\n",
      "Preparing dev data ....\n",
      "Processing 'medical' tagged data ....\n",
      "Preparing train data ....\n",
      "Preparing test data ....\n",
      "Preparing dev data ....\n",
      "Processing 'subtitles' tagged data ....\n",
      "Preparing train data ....\n",
      "Preparing test data ....\n",
      "Preparing dev data ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences done.\n",
      "20000 sentences done.\n",
      "30000 sentences done.\n",
      "40000 sentences done.\n",
      "50000 sentences done.\n",
      "60000 sentences done.\n",
      "70000 sentences done.\n",
      "80000 sentences done.\n",
      "90000 sentences done.\n",
      "100000 sentences done.\n",
      "110000 sentences done.\n",
      "120000 sentences done.\n",
      "130000 sentences done.\n",
      "140000 sentences done.\n",
      "150000 sentences done.\n",
      "160000 sentences done.\n",
      "170000 sentences done.\n",
      "180000 sentences done.\n",
      "190000 sentences done.\n",
      "200000 sentences done.\n",
      "210000 sentences done.\n",
      "220000 sentences done.\n",
      "230000 sentences done.\n",
      "240000 sentences done.\n",
      "250000 sentences done.\n",
      "260000 sentences done.\n",
      "270000 sentences done.\n",
      "280000 sentences done.\n",
      "290000 sentences done.\n",
      "300000 sentences done.\n",
      "310000 sentences done.\n",
      "320000 sentences done.\n",
      "330000 sentences done.\n",
      "340000 sentences done.\n",
      "350000 sentences done.\n",
      "360000 sentences done.\n",
      "370000 sentences done.\n",
      "380000 sentences done.\n",
      "390000 sentences done.\n",
      "400000 sentences done.\n",
      "410000 sentences done.\n",
      "420000 sentences done.\n",
      "430000 sentences done.\n",
      "440000 sentences done.\n",
      "450000 sentences done.\n",
      "460000 sentences done.\n",
      "470000 sentences done.\n",
      "480000 sentences done.\n",
      "490000 sentences done.\n",
      "500000 sentences done.\n",
      "510000 sentences done.\n",
      "520000 sentences done.\n",
      "530000 sentences done.\n",
      "540000 sentences done.\n",
      "550000 sentences done.\n",
      "560000 sentences done.\n",
      "570000 sentences done.\n",
      "580000 sentences done.\n",
      "590000 sentences done.\n",
      "600000 sentences done.\n",
      "610000 sentences done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home1/tejomay/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences done.\n"
     ]
    }
   ],
   "source": [
    "#RUN THIS FOR ORIGINAL DATASET\n",
    "\n",
    "dataset = process_data(lang='en')\n",
    "model_info = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "train_embeddings = data_to_embeddings(dataset, model_info, split='train')\n",
    "dev_embeddings = data_to_embeddings(dataset, model_info, split='dev')\n",
    "test_embeddings = data_to_embeddings(dataset, model_info, split='test')\n",
    "np.save('train/sents.npy', train_embeddings['sents'])\n",
    "np.save('train/vectors.npy', train_embeddings['vectors'])\n",
    "np.save('train/labels.npy', train_embeddings['labels'])\n",
    "np.save('dev/sents.npy', dev_embeddings['sents'])\n",
    "np.save('dev/vectors.npy', dev_embeddings['vectors'])\n",
    "np.save('dev/labels.npy', dev_embeddings['labels'])\n",
    "np.save('test/sents.npy', test_embeddings['sents'])\n",
    "np.save('test/vectors.npy', test_embeddings['vectors'])\n",
    "np.save('test/labels.npy', test_embeddings['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS FOR CFILT ENGLSIH HINDI DATA\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "# model_info = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "# # print(len(dataset['train']['translation']))\n",
    "# embeddings = data_to_embeddings(dataset, model_info, split='train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
