{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea41d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using torchaudio==0.9.0, but torchaudio>=0.10.0 is required to use MCTCTFeatureExtractor. This requires torch>=1.10.0. Please upgrade torch and torchaudio.\n",
      "You are using torch==1.9.0+cu111, but torch>=1.10.0 is required to use ViltModel. Please upgrade torch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfc1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "embeddings['sents'] = np.load('openaiGPT/sentences.npy')\n",
    "embeddings['vectors'] = np.load('openaiGPT/vectors.npy')\n",
    "embeddings['domain'] = np.load('openaiGPT/domain.npy')\n",
    "embeddings['sentiment'] = np.load('openaiGPT/sentiment.npy')\n",
    "DOMAINS = ['Automotive', 'Books', 'Music', 'Software', 'Baby']\n",
    "\n",
    "def create_known_sample(embeddings):\n",
    "    known_sample = {}\n",
    "    known_sample['sents'] = []\n",
    "    known_sample['vectors'] = []\n",
    "    known_sample['domain'] = []\n",
    "    known_sample['sentiment'] = []\n",
    "    count = 0\n",
    "    for i, domain in enumerate(embeddings['domain']):\n",
    "        count += 1\n",
    "        known_sample['sents'].append(embeddings['sents'][i])\n",
    "        known_sample['vectors'].append(embeddings['vectors'][i])\n",
    "        known_sample['domain'].append(embeddings['domain'][i])\n",
    "        known_sample['sentiment'].append(embeddings['sentiment'][i])\n",
    "        if count >= 4000:\n",
    "            break\n",
    "    embeddings['sents'] = embeddings['sents'][4000:]\n",
    "    embeddings['vectors'] = embeddings['vectors'][4000:]\n",
    "    embeddings['domain'] = embeddings['domain'][4000:]\n",
    "    embeddings['sentiment'] = embeddings['sentiment'][4000:]\n",
    "    \n",
    "    return known_sample, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b9e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_cosine_sort(embeddings, known_sample, domain='Music'):\n",
    "    count = 0\n",
    "    sorted_embeddings = {}\n",
    "    known_vector = np.zeros((embeddings['vectors'].shape[1], ))\n",
    "    for i, vector in enumerate(known_sample['vectors']):\n",
    "        if known_sample['domain'][i] == domain:\n",
    "            known_vector += vector\n",
    "            count += 1\n",
    "    known_vector /= count\n",
    "    \n",
    "    count = 1\n",
    "    score = []\n",
    "    for i, vector in enumerate(embeddings['vectors']):\n",
    "        cosine_score = cosine_similarity(vector.reshape(1,-1), known_vector.reshape(1,-1))\n",
    "#         print(cosine_score[0][0], end=\" \")\n",
    "        score.append(-1.0 * cosine_score[0][0])\n",
    "#     print(score)\n",
    "    score = np.array(score)\n",
    "    sort_index = np.argsort(score)\n",
    "    sorted_score = [-1.0 * score[i] for i in sort_index]\n",
    "#     print(sorted_score)\n",
    "    sorted_embeddings['sents'] = [embeddings['sents'][i] for i in sort_index]\n",
    "    sorted_embeddings['vectors'] = [embeddings['vectors'][i] for i in sort_index]\n",
    "    sorted_embeddings['domain'] = [embeddings['domain'][i] for i in sort_index]\n",
    "    sorted_embeddings['sentiment'] = [embeddings['sentiment'][i] for i in sort_index]\n",
    "    sorted_embeddings['score'] = sorted_score\n",
    "    return sorted_embeddings\n",
    "\n",
    "def sampling(sorted_embeddings, neg_sampling=False):\n",
    "    dataset = {}\n",
    "    dataset['sents'] = []\n",
    "    dataset['vectors'] = []\n",
    "    dataset['domain'] = []\n",
    "    dataset['sentiment'] = []\n",
    "    dataset['labels'] = []\n",
    "    for i in range(5000):\n",
    "        dataset['sents'].append(sorted_embeddings['sents'][i])\n",
    "        dataset['vectors'].append(sorted_embeddings['vectors'][i])\n",
    "        dataset['domain'].append(sorted_embeddings['domain'][i])\n",
    "        dataset['sentiment'].append(sorted_embeddings['sentiment'][i])\n",
    "        dataset['labels'].append(1)\n",
    "    \n",
    "    if not neg_sampling:\n",
    "        random_index = np.arange(5000, len(sorted_embeddings['sents']), 1)\n",
    "        np.random.shuffle(random_index)\n",
    "        random_index = random_index[:20000]\n",
    "        for idx in random_index:\n",
    "            dataset['sents'].append(sorted_embeddings['sents'][idx])\n",
    "            dataset['vectors'].append(sorted_embeddings['vectors'][idx])\n",
    "            dataset['domain'].append(sorted_embeddings['domain'][idx])\n",
    "            dataset['sentiment'].append(sorted_embeddings['sentiment'][idx])\n",
    "            dataset['labels'].append(0)\n",
    "    \n",
    "    else:\n",
    "        for i in range(1, 40001):\n",
    "            idx = -1 * i\n",
    "            if sorted_embeddings['score'][idx] < 0.4:\n",
    "                dataset['sents'].append(sorted_embeddings['sents'][idx])\n",
    "                dataset['vectors'].append(sorted_embeddings['vectors'][idx])\n",
    "                dataset['domain'].append(sorted_embeddings['domain'][idx])\n",
    "                dataset['sentiment'].append(sorted_embeddings['sentiment'][idx])\n",
    "                dataset['labels'].append(0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2d1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_P_R_F(dataset, domain):\n",
    "    tp = sum([1 for i in range(len(dataset['labels'])) if (dataset['labels'][i] == 1 and dataset['domain'][i] == domain)])\n",
    "    fp = sum([1 for i in range(len(dataset['labels'])) if (dataset['labels'][i] == 1 and dataset['domain'][i] != domain)])\n",
    "    fn = sum([1 for i in range(len(dataset['labels'])) if (dataset['labels'][i] == 0 and dataset['domain'][i] == domain)])\n",
    "#     print(tp, fp, fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    F1 = (2 * precision * recall) / (precision + recall)\n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2eea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Automotive => Precision: 0.6546 | Recall: 0.5304 | F1: 0.5860\n",
      "Domain: Books => Precision: 0.8782 | Recall: 0.6565 | F1: 0.7513\n",
      "Domain: Music => Precision: 0.9850 | Recall: 0.7060 | F1: 0.8225\n",
      "Domain: Software => Precision: 0.8606 | Recall: 0.6355 | F1: 0.7311\n",
      "Domain: Baby => Precision: 0.8192 | Recall: 0.6226 | F1: 0.7075\n"
     ]
    }
   ],
   "source": [
    "known_sample, embeddings = create_known_sample(embeddings)\n",
    "for dom in DOMAINS:\n",
    "    sorted_embeddings = domain_cosine_sort(embeddings, known_sample, domain=dom)\n",
    "    dataset = sampling(sorted_embeddings, neg_sampling=False)\n",
    "#     print(len(dataset['labels']))\n",
    "    precision, recall, F1 = domain_P_R_F(dataset, dom)\n",
    "    print(f'Domain: {dom} => Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {F1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a78de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Automotive => Precision: 0.6546 | Recall: 0.9951 | F1: 0.7897\n",
      "Domain: Books => Precision: 0.8782 | Recall: 0.9941 | F1: 0.9326\n",
      "Domain: Music => Precision: 0.9850 | Recall: 0.9972 | F1: 0.9910\n",
      "Domain: Software => Precision: 0.8606 | Recall: 0.9981 | F1: 0.9243\n",
      "Domain: Baby => Precision: 0.8192 | Recall: 0.9988 | F1: 0.9001\n"
     ]
    }
   ],
   "source": [
    "# DOMAINS = ['Automotive', 'Books', 'Music', 'Software', 'Baby']\n",
    "for dom in DOMAINS:\n",
    "    sorted_embeddings = domain_cosine_sort(embeddings, known_sample, domain=dom)\n",
    "    dataset = sampling(sorted_embeddings, neg_sampling=True)\n",
    "#     print(len(dataset['labels']))\n",
    "    precision, recall, F1 = domain_P_R_F(dataset, dom)\n",
    "    print(f'Domain: {dom} => Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {F1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b02471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# class RNN(torch.nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(RNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.input_2_hidden = torch.nn.Linear(input_size+hidden_size, hidden_size)\n",
    "#         self.input_2_output = torch.nn.Linear(input_size+hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, input_tensor, hidden_tensor):\n",
    "#         concat = torch.cat((input_tensor, hidden_tensor), 1)\n",
    "#         hidden = input_to_hidden(concat)\n",
    "#         output = input_to_output(concat)\n",
    "#         output = torch.sigmoid(output)\n",
    "#         return hidden, output\n",
    "    \n",
    "#     def init_hidden_layer(self):\n",
    "#         return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sent, sentiment, tokenizer, max_len):\n",
    "        self.sents = sent\n",
    "        self.sentiments = sentiment\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sent = self.sents[index]\n",
    "        sentiment = self.sentiments[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'sent': sent,\n",
    "            'sentiment': torch.tensor(sentiment, dtype=torch.float),\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d458c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home1/tejomay/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "known_sample, embeddings = create_known_sample(embeddings)\n",
    "sorted_embeddings = domain_cosine_sort(embeddings, known_sample, domain='Books')\n",
    "dataset = sampling(sorted_embeddings, neg_sampling=True)\n",
    "dataset['sentiment'] = [int(x) for x in dataset['sentiment']]\n",
    "\n",
    "MODEL = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "# bert = BertModel.from_pretrained(MODEL)\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35abdc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3437,)\n"
     ]
    }
   ],
   "source": [
    "X = dataset['sents']\n",
    "y = dataset['sentiment']\n",
    "\n",
    "temp = list(zip(X, y))\n",
    "random.shuffle(temp)\n",
    "X, y = zip(*temp)\n",
    "X, y = list(X), list(y)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train = np.array(X[:split])\n",
    "X_test = np.array(X[split:])\n",
    "y_train = np.array(y[:split])\n",
    "y_test = np.array(y[split:])\n",
    "\n",
    "split = int(0.85 * X_train.shape[0])\n",
    "X_val = X_train[split:]\n",
    "y_val = y_train[split:]\n",
    "X_train = X_train[:split]\n",
    "y_train = y_train[:split]\n",
    "\n",
    "print(X_train.shape)\n",
    "train_dataset = SentimentDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset = SentimentDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "test_dataset = SentimentDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "301519d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyser(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentAnalyser, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL)\n",
    "        self.dropout = torch.nn.Dropout(p=0.35)\n",
    "        self.output = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.dropout(bert_output.pooler_output)\n",
    "        out = self.output(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86872803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home1/tejomay/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home1/tejomay/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = SentimentAnalyser()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0de92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56dbce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d['sentiment']\n",
    "            targets = torch.reshape(targets, (-1,1))\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.round(outputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            correct += torch.sum(preds == targets)\n",
    "\n",
    "    return correct / n_examples, np.mean(losses)\n",
    "\n",
    "def train(model, num_epochs, train_data_loader, val_data_loader, loss_fn, optimizer, scheduler, train_size, val_size):\n",
    "    prev_val_acc = 0.0\n",
    "    for e in range(num_epochs):\n",
    "        model = model.train()\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        step = 1\n",
    "        for data in train_data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            targets = data['sentiment']\n",
    "            targets = torch.reshape(targets, (-1,1))\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.round(outputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            correct += torch.sum(preds == targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if step%100 == 0:\n",
    "                print(f'Step:{step} loss:{loss.item():.4f}')\n",
    "            step += 1\n",
    "        \n",
    "        acc = correct / train_size\n",
    "        mean_loss = np.mean(losses)\n",
    "        \n",
    "        val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, val_size)\n",
    "#         val_acc_list.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch:{e+1} Train acc:{acc:.4f} train loss:{mean_loss:.4f} || Val acc:{val_acc:.4f} prev_val_acc:{prev_val_acc:.4f}')\n",
    "        if val_acc < prev_val_acc:\n",
    "            break\n",
    "        prev_val_acc = val_acc\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671e2bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:100 loss:0.2856\n",
      "Step:200 loss:0.2159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Train acc:0.7867 train loss:0.4428 || Val acc:0.9094 prev_val_acc:0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:100 loss:0.0237\n",
      "Step:200 loss:0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 Train acc:0.9258 train loss:0.2220 || Val acc:0.9077 prev_val_acc:0.9094\n"
     ]
    }
   ],
   "source": [
    "model = train(model, num_epochs, train_data_loader, val_data_loader, loss_fn, optimizer, scheduler, len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d32ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/tejomay/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "acc, loss = eval_model(model, test_data_loader, loss_fn, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7afa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8864, device='cuda:0') 0.444679222702689\n"
     ]
    }
   ],
   "source": [
    "print(acc, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26e1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
